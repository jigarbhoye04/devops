# Phase 3.2: Analytics & Reporting Service

## Overview

Phase 3.2 implements a comprehensive analytics and reporting service using Django and Django REST Framework. This service consumes auction outcomes from Kafka, persists them to PostgreSQL, and provides a rich REST API for querying and analyzing RTB auction data.

## Architecture

```
┌─────────────────┐
│    Auction      │
│   Simulator     │
└────────┬────────┘
         │ auction_outcomes (Kafka)
         ↓
┌────────────────────────────────┐
│  Analytics Service (Consumer)  │
│  - Kafka Consumer              │
│  - Django Management Command   │
└────────┬───────────────────────┘
         │ Write
         ↓
┌────────────────────────────────┐
│  PostgreSQL Database           │
│  - outcomes_auctionoutcome     │
│  - Indexed for performance     │
└────────┬───────────────────────┘
         │ Read
         ↓
┌────────────────────────────────┐
│  Analytics Service (API)       │
│  - Django REST Framework       │
│  - /api/outcomes/*             │
└────────────────────────────────┘
```

## Components

### 1. Django Application Structure

```
analytics-service/
├── manage.py                    # Django management script
├── entrypoint.sh               # Container entrypoint
├── requirements.txt            # Python dependencies
├── Dockerfile                  # Multi-stage Docker build
├── analytics/                  # Django project
│   ├── __init__.py
│   ├── settings.py            # Configuration
│   ├── urls.py                # URL routing
│   ├── wsgi.py                # WSGI application
│   └── asgi.py                # ASGI application
└── outcomes/                   # Django app
    ├── __init__.py
    ├── apps.py                # App configuration
    ├── models.py              # AuctionOutcome model
    ├── serializers.py         # DRF serializers
    ├── views.py               # DRF viewsets
    ├── urls.py                # App URLs
    ├── admin.py               # Admin interface
    └── management/
        └── commands/
            └── process_outcomes.py  # Kafka consumer
```

### 2. AuctionOutcome Model

Complete data model with optimized indexes:

```python
class AuctionOutcome(models.Model):
    # Primary identifiers
    bid_id = models.CharField(max_length=255, db_index=True)
    user_id = models.CharField(max_length=255, db_index=True)
    site_domain = models.CharField(max_length=255, db_index=True, null=True)
    
    # Auction results
    win_status = models.BooleanField(default=False, db_index=True)
    win_price = models.DecimalField(max_digits=10, decimal_places=2)
    bid_price = models.DecimalField(max_digits=10, decimal_places=2)
    currency = models.CharField(max_length=3, default='USD')
    
    # Enrichment data
    enriched = models.BooleanField(default=False)
    user_interests = models.JSONField(default=list)
    
    # Timestamps
    timestamp = models.DateTimeField(auto_now_add=True, db_index=True)
    auction_timestamp = models.DateTimeField(null=True)
    
    # Metadata
    raw_data = models.JSONField(default=dict)
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
```

**Indexes for Performance:**
- `(-timestamp,)` - Default ordering
- `(win_status, -timestamp)` - Win/loss queries
- `(user_id, -timestamp)` - User-specific queries
- `(site_domain, -timestamp)` - Domain analysis

### 3. Kafka Consumer (Management Command)

The `process_outcomes` management command:

```bash
python manage.py process_outcomes [options]

Options:
  --max-messages N    Process N messages then exit
  --timeout MS        Consumer timeout in milliseconds
```

**Features:**
- Automatic kafka-python six compatibility patching
- Graceful error handling
- Structured JSON logging
- Transaction safety
- Configurable batch processing

### 4. REST API Endpoints

#### Core Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/outcomes/` | List all outcomes (paginated) |
| GET | `/api/outcomes/{id}/` | Retrieve specific outcome |
| GET | `/api/outcomes/stats/` | Aggregated statistics |
| GET | `/api/outcomes/winners/` | Only winning outcomes |
| GET | `/api/outcomes/daily_stats/` | Daily aggregations |

#### Query Parameters

All list endpoints support filtering:
- `user_id` - Filter by user
- `site_domain` - Filter by domain
- `win_status` - true/false
- `enriched` - true/false
- `min_price` - Minimum win price
- `max_price` - Maximum win price
- `ordering` - Sort by field (e.g., `-timestamp`, `win_price`)
- `page` - Page number
- `page_size` - Results per page (default: 100)

#### Statistics Response

```json
{
  "total_outcomes": 10000,
  "total_wins": 7000,
  "total_losses": 3000,
  "win_rate": 70.0,
  "total_revenue": 5600.00,
  "average_win_price": 0.80,
  "average_bid_price": 0.75,
  "enriched_count": 8500
}
```

## Deployment Architecture

The service runs as **two separate deployments**:

### 1. Analytics API (analytics-api-deployment)
- **Replicas**: 2 (for high availability)
- **Port**: 8000
- **Command**: `./entrypoint.sh api`
- **Purpose**: Serves REST API requests
- **Resources**: 
  - Requests: 256Mi RAM, 200m CPU
  - Limits: 512Mi RAM, 500m CPU

### 2. Analytics Consumer (analytics-consumer-deployment)
- **Replicas**: 1 (Kafka consumer group handles scaling)
- **Command**: `./entrypoint.sh consumer`
- **Purpose**: Processes Kafka messages
- **Resources**:
  - Requests: 256Mi RAM, 100m CPU
  - Limits: 512Mi RAM, 300m CPU

## Database Schema

### PostgreSQL Setup

The service uses PostgreSQL with the following configuration:

```yaml
Database: helios_analytics
User: helios
Host: postgres.helios.svc.cluster.local
Port: 5432
```

### Migrations

Django migrations are automatically applied on startup via `entrypoint.sh`:

```bash
# Database migrations are run when container starts
python manage.py migrate --noinput
```

## Configuration

### Environment Variables

All configuration is via environment variables (following Helios conventions):

```yaml
# Django Settings
DJANGO_SECRET_KEY: "production-secret-key"
DEBUG: "False"
ALLOWED_HOSTS: "*"
LOG_LEVEL: "INFO"

# Database Configuration
DB_NAME: "helios_analytics"
DB_USER: "helios"
DB_PASSWORD: "secret-from-k8s-secret"
DB_HOST: "postgres.helios.svc.cluster.local"
DB_PORT: "5432"

# Kafka Configuration
KAFKA_BROKERS: "kafka.helios.svc.cluster.local:9092"
KAFKA_TOPIC_AUCTION_OUTCOMES: "auction_outcomes"
KAFKA_CONSUMER_GROUP: "analytics-service-group"
```

### Kubernetes Secrets

Sensitive data is stored in Kubernetes secrets:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: analytics-service-secrets
type: Opaque
stringData:
  django-secret-key: "your-secret-key"
  db-password: "your-db-password"
```

## Data Flow

### Complete Pipeline (Phase 1-3.2)

```
1. HTTP Request
   ↓
2. Bid Request Handler
   ↓ [bid_requests topic]
3. Bidding Logic Service
   ├─→ User Profile Service (gRPC)
   ↓ [bid_responses topic]
4. Auction Simulator
   ↓ [auction_outcomes topic]
5. Analytics Consumer
   ↓
6. PostgreSQL Database
   ↓
7. Analytics API
   ↓
8. REST API Response
```

### Message Flow Example

**Input (auction_outcomes topic):**
```json
{
  "bid_request_id": "req-123",
  "user_id": "user-456",
  "bid_price": 0.80,
  "win_status": true,
  "win_price": 0.80,
  "currency": "USD",
  "enriched": true,
  "user_interests": ["technology", "sports"],
  "auction_timestamp": "2025-10-25T12:00:02Z"
}
```

**Stored in Database:**
```
AuctionOutcome(
  bid_id="req-123",
  user_id="user-456",
  win_status=True,
  win_price=Decimal('0.80'),
  bid_price=Decimal('0.80'),
  enriched=True,
  user_interests=["technology", "sports"],
  raw_data={...complete message...}
)
```

**API Response:**
```json
{
  "id": 1,
  "bid_id": "req-123",
  "user_id": "user-456",
  "site_domain": "",
  "win_status": true,
  "win_price": "0.80",
  "bid_price": "0.80",
  "currency": "USD",
  "enriched": true,
  "user_interests": ["technology", "sports"],
  "is_winner": true,
  "profit_margin": 0.0,
  "timestamp": "2025-10-25T12:00:03Z",
  "auction_timestamp": "2025-10-25T12:00:02Z",
  "created_at": "2025-10-25T12:00:03.123Z",
  "updated_at": "2025-10-25T12:00:03.123Z"
}
```

## Development Workflow

### Local Development

```bash
# 1. Set up environment
cd services/analytics-service
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Configure environment
export DB_HOST=localhost
export DB_PASSWORD=yourpassword
export KAFKA_BROKERS=localhost:9092

# 3. Run migrations
python manage.py migrate

# 4. Create superuser (optional)
python manage.py createsuperuser

# 5. Start dev server
python manage.py runserver

# 6. In another terminal, start consumer
python manage.py process_outcomes
```

### Testing

```bash
# Run Django tests
python manage.py test

# Test API endpoints
curl http://localhost:8000/api/outcomes/
curl http://localhost:8000/api/outcomes/stats/

# Access admin interface
# Navigate to http://localhost:8000/admin/
```

### Docker Development

```bash
# Build image
docker build -t helios/analytics-service:dev .

# Run API
docker run -p 8000:8000 \
  -e DB_HOST=host.docker.internal \
  -e DB_PASSWORD=password \
  -e KAFKA_BROKERS=host.docker.internal:9092 \
  helios/analytics-service:dev api

# Run consumer
docker run \
  -e DB_HOST=host.docker.internal \
  -e DB_PASSWORD=password \
  -e KAFKA_BROKERS=host.docker.internal:9092 \
  helios/analytics-service:dev consumer
```

## Kubernetes Deployment

### Deploy Services

```bash
# Deploy analytics service
kubectl apply -f kubernetes/services/04-analytics-service/

# Verify deployments
kubectl get deployments -n helios -l component=analytics-service

# Check pods
kubectl get pods -n helios -l component=analytics-service

# View API logs
kubectl logs -n helios -l role=api --tail=50

# Follow consumer logs
kubectl logs -n helios -l role=consumer -f
```

### Scaling

```bash
# Scale API replicas
kubectl scale deployment/analytics-api-deployment -n helios --replicas=3

# Scale consumer replicas (uses same Kafka consumer group)
kubectl scale deployment/analytics-consumer-deployment -n helios --replicas=2
```

### Database Operations

```bash
# Run migrations
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py migrate

# Create superuser
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py createsuperuser

# Access Django shell
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py shell

# Access database directly
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py dbshell
```

## API Usage Examples

### Basic Queries

```bash
# List all outcomes
curl http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/

# Get specific outcome
curl http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/1/

# Filter by user
curl "http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/?user_id=user-123"

# Get only winners
curl http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/winners/

# Filter by price range
curl "http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/?min_price=0.50&max_price=1.00"
```

### Statistics

```bash
# Overall statistics
curl http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/stats/

# Daily statistics
curl http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/daily_stats/

# Filter statistics by user
curl "http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/stats/?user_id=user-123"
```

### Pagination

```bash
# Page 2 with custom page size
curl "http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/?page=2&page_size=50"

# Ordering
curl "http://analytics-service.helios.svc.cluster.local:8000/api/outcomes/?ordering=-win_price"
```

## Monitoring & Observability

### Key Metrics

1. **Consumer Metrics**
   - Messages consumed per second
   - Processing latency
   - Error rate
   - Consumer lag

2. **API Metrics**
   - Request rate
   - Response time (p50, p95, p99)
   - Error rate (4xx, 5xx)
   - Active connections

3. **Database Metrics**
   - Query execution time
   - Connection pool usage
   - Table size growth
   - Index hit rate

### Logging

All logs are structured JSON:

```json
{
  "timestamp": "2025-10-25T12:00:00.000Z",
  "level": "INFO",
  "message": "Auction outcome saved",
  "bid_id": "req-123",
  "win_status": true,
  "win_price": "0.80"
}
```

### Health Checks

```bash
# API health
curl http://analytics-service:8000/api/outcomes/

# Database health (from pod)
python manage.py dbshell -c '\conninfo'

# Consumer health (check logs)
kubectl logs -n helios -l role=consumer --tail=1
```

## Troubleshooting

### Common Issues

#### 1. Consumer Not Processing Messages

**Symptoms**: No new outcomes in database

**Diagnosis**:
```bash
# Check consumer logs
kubectl logs -n helios -l role=consumer --tail=100

# Check Kafka topic
kubectl run kafka-consumer --rm -i --restart=Never \
  --image=confluentinc/cp-kafka:latest -n helios -- \
  kafka-console-consumer --bootstrap-server kafka:9092 \
  --topic auction_outcomes --from-beginning --max-messages 10
```

**Solution**:
```bash
# Restart consumer
kubectl rollout restart deployment/analytics-consumer-deployment -n helios

# Check environment variables
kubectl get deployment analytics-consumer-deployment -n helios \
  -o jsonpath='{.spec.template.spec.containers[0].env}'
```

#### 2. Database Connection Errors

**Symptoms**: API returns 500 errors, consumer crashes

**Diagnosis**:
```bash
# Check database connectivity
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py dbshell

# Check database credentials
kubectl get secret analytics-service-secrets -n helios -o yaml
```

**Solution**:
```bash
# Verify PostgreSQL is running
kubectl get pods -n helios -l app=postgres

# Update database password
kubectl create secret generic analytics-service-secrets -n helios \
  --from-literal=db-password=newpassword \
  --dry-run=client -o yaml | kubectl apply -f -

# Restart deployments
kubectl rollout restart deployment/analytics-api-deployment -n helios
kubectl rollout restart deployment/analytics-consumer-deployment -n helios
```

#### 3. Migration Errors

**Symptoms**: Database schema mismatch

**Solution**:
```bash
# Check migration status
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py showmigrations

# Run migrations
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py migrate

# If needed, reset migrations (CAREFUL!)
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py migrate outcomes zero
kubectl exec -it deployment/analytics-api-deployment -n helios -- \
  python manage.py migrate
```

## Performance Optimization

### Database Tuning

```python
# Add custom indexes
class Meta:
    indexes = [
        models.Index(fields=['site_domain', 'win_status', '-timestamp']),
        # Add more as needed
    ]

# Use select_related for foreign keys
queryset = AuctionOutcome.objects.select_related('related_model')

# Use only() to limit fields
queryset = AuctionOutcome.objects.only('bid_id', 'win_status', 'win_price')
```

### API Performance

```python
# Pagination settings
REST_FRAMEWORK = {
    'PAGE_SIZE': 100,  # Adjust based on needs
    'MAX_PAGE_SIZE': 1000,
}

# Enable caching
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.redis.RedisCache',
        'LOCATION': 'redis://redis:6379/1',
    }
}
```

### Consumer Performance

```bash
# Increase consumer replicas
kubectl scale deployment/analytics-consumer-deployment -n helios --replicas=3

# Tune Kafka consumer settings
# In process_outcomes.py, adjust:
# - fetch_min_bytes
# - fetch_max_wait_ms
# - max_partition_fetch_bytes
```

## Security Best Practices

1. **Secret Management**
   - Use Kubernetes secrets for sensitive data
   - Rotate secrets regularly
   - Never commit secrets to code

2. **Database Security**
   - Use strong passwords
   - Limit database user permissions
   - Enable SSL/TLS for connections

3. **API Security**
   - Set DEBUG=False in production
   - Use ALLOWED_HOSTS restrictions
   - Consider adding authentication/authorization
   - Enable HTTPS/TLS

4. **Django Security**
   - Keep Django updated
   - Use CSRF protection
   - Configure SECURE_* settings
   - Enable security middleware

## Success Criteria

- ✅ Django application created with proper structure
- ✅ AuctionOutcome model with all required fields
- ✅ Kafka consumer management command implemented
- ✅ REST API with CRUD operations
- ✅ Statistics and aggregation endpoints
- ✅ Separate API and consumer deployments
- ✅ Environment-based configuration
- ✅ Structured JSON logging
- ✅ Database migrations working
- ✅ API responding with 200 OK

## Next Steps

### Phase 4: Advanced Analytics
- [ ] Implement real-time dashboards
- [ ] Add Grafana visualizations
- [ ] Implement data aggregation jobs
- [ ] Add ML-based bid optimization
- [ ] Implement A/B testing framework

### Future Enhancements
- [ ] Add GraphQL API
- [ ] Implement data export (CSV, Excel)
- [ ] Add report scheduling
- [ ] Implement data retention policies
- [ ] Add Redis caching layer
- [ ] Implement WebSocket real-time updates

## Files Created/Modified

### New Files
- `analytics/settings.py` - Django configuration
- `analytics/urls.py` - URL routing
- `analytics/wsgi.py` - WSGI application
- `outcomes/models.py` - AuctionOutcome model
- `outcomes/serializers.py` - DRF serializers
- `outcomes/views.py` - DRF viewsets
- `outcomes/urls.py` - App URLs
- `outcomes/admin.py` - Django admin
- `outcomes/management/commands/process_outcomes.py` - Kafka consumer
- `manage.py` - Django CLI
- `entrypoint.sh` - Container entrypoint
- `services/analytics-service/README.md` - Service documentation

### Modified Files
- `requirements.txt` - Updated dependencies
- `Dockerfile` - Multi-stage Django build
- `kubernetes/services/04-analytics-service/deployment.yaml` - Dual deployments
- `kubernetes/services/04-analytics-service/service.yaml` - Updated service

## Conclusion

Phase 3.2 completes the analytics pipeline with a production-ready Django service that:

1. Consumes auction outcomes from Kafka
2. Persists data to PostgreSQL with optimized schema
3. Provides comprehensive REST API for data access
4. Includes built-in analytics and aggregations
5. Follows Helios conventions for configuration and logging
6. Scales independently (API vs. consumer)
7. Includes comprehensive documentation and testing tools

The system now has a complete data pipeline from bid request ingestion through to persistent storage with queryable API access for reporting and analytics.
